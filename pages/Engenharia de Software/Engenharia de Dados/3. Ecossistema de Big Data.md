# Introdução

- Cresicmento na geração de dados
	- Redes sociais
	- Sensores IoT
	- Dados de satélite
	- Dados geogra´ficos
	- Dados de plataformas de streaming
- Volumens massivos de dados qeu ecedem a capacidade de tecnologias típicas como SGBDs relacionais
- Volume de dados na casa dos terabytes, petabytes, etc

# Características

- **Volume**: Refere-se ao enorme volume de dados gerados
- **Velocidade**: Refere-se à velocidade na criação dos dados, na qual são gerados e na qual devem ser processados
- **Variedade**: Refere-se à variedade dos tipos de fontes de dados (estruturados, semiestruturados e não estruturados)
- **Veracidade**: Refere-se à credibilidade das fontes de dados
- **Valor**: Refere-se ao valor agregado pelos dados para a organização

# Ecossistema

## Hadoop

Framework composta pelo HDFS (Hadoop Distributed File System) e pelo modelo de programação MapReduce

Alguns projetos implementados com funcionalidades adicionais:
- **Apache Pig**: Interface de alto nível para interação com o Hadoop (utilizando consultas semelhantes ao SQL), abstraindo a complexidade do MapReduce
- **Apache Hive**: Solução de Data Warehouses ([[2. Data Warehouses e Data Lakes]]) sobre o Hadoop

![[Hadoop composition diagram.webp]]
[Fonte](https://www.projectpro.io/article/hadoop-ecosystem-components-and-its-architecture/114)

### HDFS

- Projetado para executar em clusters de hardware barato e abundante
- Provê acesso rápido a grande volume de dados
- Dados replicados para confiabilidade e disponibilidade

### MapReduce

Modelo de programação caracterizado por duas etapas principais:

**Map**
- Divide o problema para ser paralelizado em diversas máquinas

**Reduce**
- Reúne as soluções parciais

![[Hadoop MapReduce diagram.webp]]
[Fonte](https://www.guru99.com/introduction-to-mapreduce.html)

## Spark

Framework para processamento de Big Data construído com foco em velocidade, facilidade de uso e análises sofisticadas. Ele utiliza cache em memória e execução de consultas otimizadas para consultas rápidas em dados de qualquer tamanho.

- Processa grandes volumes de dados de forma paralela e distribuída
- Componentes para diferentes tipos de processamento, disponibilizados sobre o núcleo
- Spark Streamming: Processamento de dados em tempo real
- GraphX: Processamento de grafos
- SparkSQL: SQL para realização de consultas
- MLlib: Biblioteca de aprendizado de máquina